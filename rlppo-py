import os
import pygame
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Normal
import gymnasium as gym
from gymnasium import spaces
import random
import matplotlib.pyplot as plt
from collections import deque

# Constants
X_LIMIT = 800
Y_LIMIT = 800
WOLF_SPEED_LIMIT = 10.0         # Soglia aggiornata per la velocità del lupo
WOLF_ACCEL_LIMIT = 3.0          # Soglia aggiornata per l'accelerazione del lupo
WOLF_MAGNITUDE_SCALING = 0.5
WOLF_ANGLE_SCALING = 0.5
SHEEP_SPEED_LIMIT = 1.0
SHEEP_ACCEL_LIMIT = 1.0
SHEEP_MAGNITUDE_SCALING = 0.5
SHEEP_ANGLE_SCALING = 0.5
WOLF_INITIAL_ENERGY = 1000
WOLVES_RADIUS = 3
SHEEPS_RADIUS = 3
SPEED_DELTA_ANGLE_THRESHOLD = 2*np.pi/16
MAGNITUDE_THRESHOLD = 8
BIG_NUMBER = 100000000.0

BACKGROUND_COLOR = (125, 125, 125)
WOLVES_COLOR = (0, 0, 0)
SHEEPS_COLOR = (255, 255, 255)

NUM_SHEEPS = 1

# Costante per la normalizzazione della distanza toroidale (massima distanza)
MAX_TOROIDAL_DISTANCE = np.sqrt((X_LIMIT/2)**2 + (Y_LIMIT/2)**2)

DISTANCE_PENALTY_COEF = 0.05       # Coefficiente di penalizzazione

# Hyperparameters
LEARNING_RATE = 3e-4
GAMMA = 0.99
GAE_LAMBDA = 0.95
CLIP_EPSILON = 0.2
EPOCHS = 10
BATCH_SIZE = 64
ENTROPY_COEF = 0.01
VALUE_COEF = 0.5
MAX_GRAD_NORM = 0.5
NUM_STEPS = 2048  # Steps per update
BUFFER_SIZE = 10000

# Training settings
NUM_EPISODES = 5000
SAVE_INTERVAL = 50
EVAL_INTERVAL = 1
RENDER_TRAINING = False
EVAL_EPISODES = 5

# Checkpoint settings
CHECKPOINT_PATH = None

# Evaluation only mode
EVAL_ONLY = False

# Initialize PyGame
pygame.init()
screen = pygame.display.set_mode((X_LIMIT, Y_LIMIT))
pygame.font.init()
myfont = pygame.font.SysFont('Comic Sans MS', 12)


class Sheep:
    def __init__(self):
        self.reset()

    def reset(self):
        self.x = random.randrange(0, X_LIMIT)
        self.y = random.randrange(0, Y_LIMIT)
        self.x_speed = random.random() * SHEEP_SPEED_LIMIT
        self.y_speed = random.random() * SHEEP_SPEED_LIMIT
        self.magnitude_accel = 0.0
        self.angle_accel = 0.0

    def update(self, magnitude_accel_delta, angle_accel_delta):
        magnitude_accel_delta = (magnitude_accel_delta * 2 - 1) * SHEEP_MAGNITUDE_SCALING
        angle_accel_delta = (angle_accel_delta * 2 - 1) * SHEEP_ANGLE_SCALING

        self.magnitude_accel += magnitude_accel_delta
        self.magnitude_accel = np.clip(self.magnitude_accel, 0.0, SHEEP_ACCEL_LIMIT)

        self.angle_accel += angle_accel_delta
        if self.angle_accel > np.pi:
            self.angle_accel -= 2*np.pi
        if self.angle_accel < -np.pi:
            self.angle_accel += 2*np.pi

        x_accel = self.magnitude_accel * np.cos(self.angle_accel)
        y_accel = self.magnitude_accel * np.sin(self.angle_accel)

        self.x_speed += x_accel
        self.y_speed += y_accel

        magnitude_speed = np.sqrt(self.x_speed**2 + self.y_speed**2)
        if magnitude_speed > SHEEP_SPEED_LIMIT:
            self.x_speed *= SHEEP_SPEED_LIMIT / magnitude_speed
            self.y_speed *= SHEEP_SPEED_LIMIT / magnitude_speed

        self.x += self.x_speed
        if self.x > X_LIMIT:
            self.x = 0
        if self.x < 0:
            self.x = X_LIMIT - 1

        self.y += self.y_speed
        if self.y > Y_LIMIT:
            self.y = 0
        if self.y < 0:
            self.y = Y_LIMIT - 1

    def draw(self, screen):
        pygame.draw.circle(screen, SHEEPS_COLOR, (int(self.x), int(self.y)), SHEEPS_RADIUS)
        pygame.draw.line(screen, SHEEPS_COLOR, (int(self.x), int(self.y)),
                         (int(self.x + self.x_speed), int(self.y + self.y_speed)))


class Wolf:
    def __init__(self, id=0):
        self.x = random.randrange(0, X_LIMIT)
        self.y = random.randrange(0, Y_LIMIT)
        self.x_speed = random.random() * WOLF_SPEED_LIMIT
        self.y_speed = random.random() * WOLF_SPEED_LIMIT
        self.magnitude_accel = 0.0
        self.angle_accel = 0.0
        self.x_direction = self.x_speed
        self.y_direction = self.y_speed
        self.energy = WOLF_INITIAL_ENERGY
        self.id = id
        self.best = False

    def update(self, magnitude_accel_delta, angle_accel_delta):
        # Scala in modo non lineare per evitare saturazione
        magnitude_accel_delta = np.tanh(magnitude_accel_delta * 0.8) * 0.95
        angle_accel_delta = np.tanh(angle_accel_delta * 0.8) * 0.95

        magnitude_accel_delta = np.clip(magnitude_accel_delta, -1, 1)
        angle_accel_delta = np.clip(angle_accel_delta, -1, 1)

        magnitude_accel_delta *= WOLF_MAGNITUDE_SCALING
        angle_accel_delta *= WOLF_ANGLE_SCALING

        self.magnitude_accel += magnitude_accel_delta
        self.magnitude_accel = np.clip(self.magnitude_accel, 0.0, WOLF_ACCEL_LIMIT)

        self.angle_accel += angle_accel_delta
        if self.angle_accel > np.pi:
            self.angle_accel -= 2*np.pi
        if self.angle_accel < -np.pi:
            self.angle_accel += 2*np.pi

        x_accel = self.magnitude_accel * np.cos(self.angle_accel)
        y_accel = self.magnitude_accel * np.sin(self.angle_accel)

        self.x_speed += x_accel
        self.y_speed += y_accel

        magnitude_speed = np.sqrt(self.x_speed**2 + self.y_speed**2)
        # Penalità progressiva vicino al limite di velocità
        if magnitude_speed > WOLF_SPEED_LIMIT * 0.8:
            scaling_factor = 1.0 - 0.5 * ((magnitude_speed - WOLF_SPEED_LIMIT * 0.8) / (WOLF_SPEED_LIMIT * 0.2))
            scaling_factor = max(scaling_factor, WOLF_SPEED_LIMIT / magnitude_speed)
            self.x_speed *= scaling_factor
            self.y_speed *= scaling_factor

        if self.x_speed != 0.0 or self.y_speed != 0.0:
            self.x_direction = self.x_speed
            self.y_direction = self.y_speed

        self.x += self.x_speed
        if self.x > X_LIMIT:
            self.x = 0
        if self.x < 0:
            self.x = X_LIMIT - 1

        self.y += self.y_speed
        if self.y > Y_LIMIT:
            self.y = 0
        if self.y < 0:
            self.y = Y_LIMIT - 1

    def draw(self, screen):
        color = (255, 255, 0) if self.best else WOLVES_COLOR
        pygame.draw.circle(screen, color, (int(self.x), int(self.y)), WOLVES_RADIUS)
        pygame.draw.line(screen, color, (int(self.x), int(self.y)),
                         (int(self.x + self.x_speed), int(self.y + self.y_speed)))


class WolfSheepEnv(gym.Env):
    def __init__(self, num_sheeps=NUM_SHEEPS, render_mode=None):
        super().__init__()
        self.num_sheeps = num_sheeps
        self.render_mode = render_mode

        # Definizione dello spazio delle azioni
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)

        # Espansione dello spazio di osservazione per includere le informazioni sulle 3 pecore più vicine
        self.NUM_NEAREST_SHEEPS = 3
        obs_dim = 2 + self.NUM_NEAREST_SHEEPS * 3
        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(obs_dim,), dtype=np.float32)

        self.wolf = Wolf()
        self.sheeps = []
        self.reset()

        self.steps = 0
        self.max_steps = 1000

    def reset(self, seed=None):
        super().reset(seed=seed)
        self.steps = 0
        self.wolf = Wolf()
        self.sheeps = [Sheep() for _ in range(self.num_sheeps)]
        return self._get_obs(), {}

    def _get_obs(self):
        """Costruisce l'osservazione per il lupo includendo informazioni su più pecore vicine."""
        # Informazioni sullo stato del lupo
        wolf_speed = np.linalg.norm([self.wolf.x_speed, self.wolf.y_speed])
        wolf_speed_normalized = wolf_speed / WOLF_SPEED_LIMIT
        wolf_accel_normalized = self.wolf.magnitude_accel / WOLF_ACCEL_LIMIT

        # Calcoliamo per ogni pecora la distanza toroidale e le differenze (delta_x, delta_y)
        sheep_infos = []
        for sheep in self.sheeps:
            delta_x = sheep.x - self.wolf.x
            if delta_x > X_LIMIT/2:
                delta_x -= X_LIMIT
            if delta_x < -X_LIMIT/2:
                delta_x += X_LIMIT

            delta_y = sheep.y - self.wolf.y
            if delta_y > Y_LIMIT/2:
                delta_y -= Y_LIMIT
            if delta_y < -Y_LIMIT/2:
                delta_y += Y_LIMIT

            distance = np.linalg.norm([delta_x, delta_y])
            normalized_delta_x = delta_x / (X_LIMIT/2)
            normalized_delta_y = delta_y / (Y_LIMIT/2)
            normalized_distance = distance / MAX_TOROIDAL_DISTANCE

            sheep_infos.append((distance, normalized_delta_x, normalized_delta_y, normalized_distance))

        # Ordiniamo le pecore per distanza crescente
        sheep_infos.sort(key=lambda x: x[0])
        nearest = sheep_infos[:self.NUM_NEAREST_SHEEPS]

        # Se non ci sono abbastanza pecore, pad con zeri
        while len(nearest) < self.NUM_NEAREST_SHEEPS:
            nearest.append((0.0, 0.0, 0.0, 0.0))

        obs = [wolf_speed_normalized, wolf_accel_normalized]
        for _, ndx, ndy, ndist in nearest:
            obs.extend([ndx, ndy, ndist])

        return np.array(obs, dtype=np.float32)

    def step(self, action):
        self.steps += 1

        # Salva la posizione del lupo prima dell'aggiornamento
        old_x, old_y = self.wolf.x, self.wolf.y

        # Aggiorna il lupo e le pecore
        self.wolf.update(action[0], action[1])
        for sheep in self.sheeps:
            sheep.update(random.random(), random.random())

        # Calcola la distanza percorsa (tenendo conto dell'effetto toroidale)
        dx = abs(self.wolf.x - old_x)
        if dx > X_LIMIT / 2:
            dx = X_LIMIT - dx
        dy = abs(self.wolf.y - old_y)
        if dy > Y_LIMIT / 2:
            dy = Y_LIMIT - dy
        displacement = np.sqrt(dx**2 + dy**2)

        # Penalità direttamente proporzionale alla distanza percorsa
        distance_penalty = DISTANCE_PENALTY_COEF * displacement

        # Calcola il reward basato sulla vicinanza con le pecore
        min_magnitude = BIG_NUMBER
        closest_sheep = None
        for sheep in self.sheeps:
            delta_x = sheep.x - self.wolf.x
            if delta_x > X_LIMIT/2:
                delta_x -= X_LIMIT
            if delta_x < -X_LIMIT/2:
                delta_x += X_LIMIT

            delta_y = sheep.y - self.wolf.y
            if delta_y > Y_LIMIT/2:
                delta_y -= Y_LIMIT
            if delta_y < -Y_LIMIT/2:
                delta_y += Y_LIMIT

            magnitude = np.linalg.norm([delta_x, delta_y])
            if magnitude < min_magnitude:
                min_magnitude = magnitude
                closest_sheep = sheep

        reward = -0.01
        proximity_reward = 1.0 / (1.0 + min_magnitude / 50)
        reward += proximity_reward * 0.5

        if min_magnitude < MAGNITUDE_THRESHOLD:
            reward += 10.0
            if closest_sheep:
                closest_sheep.reset()

        wolf_speed = np.linalg.norm([self.wolf.x_speed, self.wolf.y_speed])
        speed_limit_penalty = 0.0
        if wolf_speed > WOLF_SPEED_LIMIT * 0.85:
            proximity_to_limit = (wolf_speed - WOLF_SPEED_LIMIT * 0.85) / (WOLF_SPEED_LIMIT * 0.15)
            speed_limit_penalty = (proximity_to_limit ** 2) * 0.1
        reward -= speed_limit_penalty

        # Sottrai la penalità proporzionale alla distanza percorsa
        reward -= distance_penalty

        done = self.steps >= self.max_steps
        return self._get_obs(), reward, done, False, {
            "magnitude": min_magnitude,
            "wolf_speed": wolf_speed,
            "speed_limit_penalty": speed_limit_penalty,
            "distance_penalty": distance_penalty
        }

    def render(self):
        if self.render_mode is None:
            return

        screen.fill(BACKGROUND_COLOR)
        self.wolf.draw(screen)
        for sheep in self.sheeps:
            sheep.draw(screen)
        text = myfont.render(f"Steps: {self.steps}", False, (0, 0, 0))
        screen.blit(text, (10, 10))
        pygame.display.update()


# PPO Actor-Critic Networks con architettura potenziata
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, device=None):
        super(ActorCritic, self).__init__()
        self.device = device if device is not None else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Rete condivisa con 3 layer e 128 neuroni ciascuno
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        
        # Rete per la policy (Actor)
        self.actor_mean = nn.Linear(128, action_dim)
        nn.init.xavier_uniform_(self.actor_mean.weight, gain=0.01)
        nn.init.constant_(self.actor_mean.bias, 0.0)
        initial_log_std = np.log(1.0)  # Esplorazione iniziale aumentata
        self.actor_log_std = nn.Parameter(torch.ones(action_dim) * initial_log_std)
        
        # Rete per il valore (Critic) con un ulteriore layer
        self.critic = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        
        self.to(self.device)
        
    def forward(self, state):
        shared_features = self.shared(state)
        action_mean = self.actor_mean(shared_features)
        action_std = self.actor_log_std.exp()
        dist = Normal(action_mean, action_std)
        value = self.critic(shared_features)
        return dist, value
    
    def get_action(self, state, deterministic=False):
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        dist, _ = self.forward(state)
        action = dist.mean if deterministic else dist.sample()
        return action.squeeze().detach().cpu().numpy()
    
    def evaluate(self, states, actions):
        dist, values = self.forward(states)
        action_log_probs = dist.log_prob(actions).sum(dim=1, keepdim=True)
        entropy = dist.entropy().sum(dim=1, keepdim=True)
        return action_log_probs, values, entropy


class PPOAgent:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.policy = ActorCritic(state_dim, action_dim, device=self.device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=LEARNING_RATE)
        
        self.reset_buffers()
        self.reward_history = []
        self.avg_length = []
        
        self.entropy_coef = ENTROPY_COEF * 10.0  # Esplorazione iniziale potenziata
        self.min_entropy_coef = ENTROPY_COEF
        self.entropy_decay = 0.998
        self.updates_count = 0
        
        self.action_noise = 0.3
        self.noise_decay = 0.998
        
    def reset_buffers(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        
    def remember(self, state, action, reward, value, log_prob, done):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.log_probs.append(log_prob)
        self.dones.append(done)
        
    def get_action(self, state, deterministic=False):
        if deterministic:
            raw_action = self.policy.get_action(state, deterministic=True)
        else:
            raw_action = self.policy.get_action(state, deterministic=False)
            if self.updates_count < 50:
                current_noise = self.action_noise * (self.noise_decay ** self.updates_count)
                noise = np.random.normal(0, current_noise, size=self.action_dim)
                raw_action = raw_action + noise
        raw_action = np.clip(raw_action, -1.0, 1.0)
        scaled_action = np.tanh(raw_action * 0.7) * 0.9
        return scaled_action
        
    def compute_gae(self, next_value):
        rewards = np.array(self.rewards)
        values = np.append(np.array(self.values), next_value)
        dones = np.array(self.dones)
        
        advantages = np.zeros_like(rewards)
        gae = 0
        for t in reversed(range(len(rewards))):
            next_non_terminal = 1.0 - dones[t]
            next_val = values[t + 1]
            delta = rewards[t] + GAMMA * next_val * next_non_terminal - values[t]
            gae = delta + GAMMA * GAE_LAMBDA * next_non_terminal * gae
            advantages[t] = gae
        returns = advantages + np.array(self.values)
        return advantages, returns
        
    def update(self):
        self.updates_count += 1
        self.entropy_coef = max(self.min_entropy_coef, self.entropy_coef * self.entropy_decay)
        
        state = torch.FloatTensor(self.states[-1]).to(self.device)
        with torch.no_grad():
            _, next_value = self.policy(state)
            next_value = next_value.item()
            
        advantages, returns = self.compute_gae(next_value)
        
        states = torch.FloatTensor(np.array(self.states)).to(self.device)
        actions = torch.FloatTensor(np.array(self.actions)).to(self.device)
        old_log_probs = torch.FloatTensor(np.array(self.log_probs)).to(self.device)
        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = torch.FloatTensor(returns).to(self.device)
        
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        batch_size = min(BATCH_SIZE, len(self.states))
        indices = np.arange(len(self.states))
        
        for _ in range(EPOCHS):
            np.random.shuffle(indices)
            for start_idx in range(0, len(self.states), batch_size):
                end_idx = min(start_idx + batch_size, len(self.states))
                mb_indices = indices[start_idx:end_idx]
                mb_states = states[mb_indices]
                mb_actions = actions[mb_indices]
                mb_old_log_probs = old_log_probs[mb_indices]
                mb_advantages = advantages[mb_indices]
                mb_returns = returns[mb_indices]
                
                new_log_probs, values, entropy = self.policy.evaluate(mb_states, mb_actions)
                ratio = torch.exp(new_log_probs - mb_old_log_probs)
                surr1 = ratio * mb_advantages
                surr2 = torch.clamp(ratio, 1.0 - CLIP_EPSILON, 1.0 + CLIP_EPSILON) * mb_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                mb_returns = mb_returns.view(-1, 1)
                value_loss = nn.MSELoss()(values, mb_returns)
                entropy_loss = -entropy.mean()
                
                current_entropy_coef = self.entropy_coef
                if self.updates_count < 100:
                    exploration_bonus = max(0, 100 - self.updates_count) / 100.0
                    current_entropy_coef *= (1.0 + exploration_bonus)
                
                loss = policy_loss + VALUE_COEF * value_loss - current_entropy_coef * entropy_loss
                
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.policy.parameters(), MAX_GRAD_NORM)
                self.optimizer.step()
                
        if self.updates_count % 10 == 0:
            print(f"Current entropy coefficient: {self.entropy_coef:.5f}, Noise level: {self.action_noise * (self.noise_decay ** self.updates_count):.5f}")
        self.reset_buffers()
        
    def save(self, filename, **kwargs):
        checkpoint = {
            'model_state_dict': self.policy.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'entropy_coef': self.entropy_coef,
            'updates_count': self.updates_count,
            'action_noise': self.action_noise,
            'noise_decay': self.noise_decay
        }
        for key, value in kwargs.items():
            checkpoint[key] = value
        torch.save(checkpoint, filename)
        
    def load(self, checkpoint_path):
        try:
            checkpoint = torch.load(checkpoint_path)
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                self.policy.load_state_dict(checkpoint['model_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                if 'entropy_coef' in checkpoint:
                    self.entropy_coef = checkpoint['entropy_coef']
                if 'updates_count' in checkpoint:
                    self.updates_count = checkpoint['updates_count']
                if 'action_noise' in checkpoint:
                    self.action_noise = checkpoint['action_noise']
                if 'noise_decay' in checkpoint:
                    self.noise_decay = checkpoint['noise_decay']
                return checkpoint
            else:
                self.policy.load_state_dict(checkpoint)
                return {}
        except Exception as e:
            print(f"Error loading checkpoint: {e}")
            return {}


def train_ppo(num_episodes=NUM_EPISODES, save_interval=SAVE_INTERVAL, eval_interval=EVAL_INTERVAL, 
              render_training=RENDER_TRAINING, checkpoint_path=CHECKPOINT_PATH):
    env = WolfSheepEnv(num_sheeps=NUM_SHEEPS, render_mode=None)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    
    agent = PPOAgent(state_dim, action_dim)
    start_episode = 1
    best_mean_reward = -float('inf')
    episode_rewards = []
    
    if checkpoint_path:
        print(f"Loading model from checkpoint: {checkpoint_path}")
        checkpoint = agent.load(checkpoint_path)
        if 'episode' in checkpoint:
            start_episode = checkpoint['episode'] + 1
            print(f"Resuming training from episode {start_episode}")
        if 'best_mean_reward' in checkpoint:
            best_mean_reward = checkpoint['best_mean_reward']
            print(f"Previous best mean reward: {best_mean_reward:.2f}")
        if 'episode_rewards' in checkpoint:
            episode_rewards = checkpoint['episode_rewards']
            print(f"Loaded {len(episode_rewards)} previous episode rewards")
        print(f"Entropy coefficient: {agent.entropy_coef:.5f}")
        print(f"Updates count: {agent.updates_count}")
        print("Checkpoint loaded successfully!")
    
    os.makedirs("ppo_models", exist_ok=True)
    mean_rewards = deque(maxlen=100)
    if len(episode_rewards) > 0:
        for r in episode_rewards[-100:]:
            mean_rewards.append(r)
    
    for episode in range(start_episode, num_episodes + 1):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        
        while not done:
            action = agent.get_action(state)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
            with torch.no_grad():
                dist, value = agent.policy(state_tensor)
                action_tensor = torch.FloatTensor(action).to(agent.device)
                log_prob = dist.log_prob(action_tensor).sum().item()
            next_state, reward, done, _, _ = env.step(action)
            agent.remember(state, action, reward, value.item(), log_prob, done)
            state = next_state
            episode_reward += reward
            if render_training:
                env.render()
            if len(agent.states) >= NUM_STEPS or done:
                agent.update()
        
        episode_rewards.append(episode_reward)
        mean_rewards.append(episode_reward)
        mean_reward = np.mean(mean_rewards)
        
        if mean_reward > best_mean_reward and episode > 100:
            best_mean_reward = mean_reward
            agent.save("ppo_models/best_model.pth", 
                       episode=episode, 
                       best_mean_reward=best_mean_reward,
                       episode_rewards=episode_rewards)
            print(f"New best model saved with mean reward: {best_mean_reward:.2f}")
        
        if episode % save_interval == 0:
            agent.save(f"ppo_models/model_{episode}.pth",
                       episode=episode,
                       best_mean_reward=best_mean_reward,
                       episode_rewards=episode_rewards)
            plt.figure(figsize=(10, 5))
            plt.plot(episode_rewards)
            plt.title("Episode Rewards")
            plt.xlabel("Episode")
            plt.ylabel("Reward")
            plt.savefig(f"ppo_models/rewards_{episode}.png")
            plt.close()
        
        if episode % eval_interval == 0:
            print(f"\n--- Evaluating model at episode {episode} ---")
            eval_env = WolfSheepEnv(num_sheeps=NUM_SHEEPS, render_mode="human")
            eval_rewards = []
            eval_state, _ = eval_env.reset()
            eval_reward = 0
            eval_done = False
            while not eval_done:
                eval_action = agent.get_action(eval_state, deterministic=True)
                eval_next_state, eval_rew, eval_done, _, _ = eval_env.step(eval_action)
                eval_state = eval_next_state
                eval_reward += eval_rew
                eval_env.render()
                pygame.time.delay(5)
            eval_rewards.append(eval_reward)
            print(f"Evaluation Episode, Reward: {eval_reward:.2f}")
            print(f"Average Evaluation Reward: {np.mean(eval_rewards):.2f}")
            print("--- Evaluation complete ---\n")
        
        print(f"Episode: {episode}, Reward: {episode_reward:.2f}, Mean Reward: {mean_reward:.2f}")
    
    agent.save("ppo_models/final_model.pth")
    plt.figure(figsize=(10, 5))
    plt.plot(episode_rewards)
    plt.title("Episode Rewards")
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.savefig("ppo_models/final_rewards.png")
    plt.close()
    pygame.quit()
    return agent


def evaluate_ppo(model_path, num_episodes=10, render=True):
    env = WolfSheepEnv(num_sheeps=NUM_SHEEPS, render_mode="human" if render else None)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    agent = PPOAgent(state_dim, action_dim)
    agent.load(model_path)
    
    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        while not done:
            action = agent.get_action(state, deterministic=True)
            next_state, reward, done, _, _ = env.step(action)
            state = next_state
            episode_reward += reward
            if render:
                env.render()
                pygame.time.delay(30)
        print(f"Evaluation Episode {episode+1}/{num_episodes}, Reward: {episode_reward:.2f}")
    
    pygame.quit()


if __name__ == "__main__":
    agent = train_ppo(num_episodes=NUM_EPISODES, save_interval=SAVE_INTERVAL, 
                      eval_interval=EVAL_INTERVAL, render_training=RENDER_TRAINING, 
                      checkpoint_path=CHECKPOINT_PATH)
    print("\n--- Final Evaluation of Best Model ---")
    evaluate_ppo("ppo_models/best_model.pth", num_episodes=5, render=True)
